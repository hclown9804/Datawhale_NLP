{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯\n",
    " \n",
    "贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。而朴素朴素贝叶斯分类是贝叶斯分类中最简单，也是常见的一种分类方法。\n",
    "\n",
    "核心算法：（贝叶斯公式）\n",
    "![贝叶斯公式](https://ws4.sinaimg.cn/large/006tKfTcly1g0wkdfa6rrj307l028aa1.jpg)\n",
    "\n",
    "![其他形式](https://ws3.sinaimg.cn/large/006tKfTcly1g0wkeey1nxj30ey02mdg1.jpg)\n",
    "\n",
    "基于朴素贝叶斯公式，比较出后验概率的最大值来进行分类，后验概率的计算是由先验概率与类条件概率的乘积得出，先验概率和类条件概率要通过训练数据集得出，即为朴素贝叶斯分类模型，将其保存为中间结果，测试文档进行分类时调用这个中间结果得出后验概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import time\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "\n",
    "train = pd.read_csv('./data/train_set.csv')[:1000]\n",
    "test = pd.read_csv('./data/test_set.csv')[:100]\n",
    "\n",
    "column=\"word_seg\"\n",
    "n = train.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2),min_df=3, max_df=0.9,use_idf=1,smooth_idf=1, sublinear_tf=1)\n",
    "trn_term_doc = vec.fit_transform(train[column])\n",
    "test_term_doc = vec.transform(test[column])\n",
    "\n",
    "y = (train[\"class\"]-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>word_seg</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7368 1252069 365865 755561 1044285 129532 1053...</td>\n",
       "      <td>816903 597526 520477 1179558 1033823 758724 63...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>581131 165432 7368 957317 1197553 570900 33659...</td>\n",
       "      <td>90540 816903 441039 816903 569138 816903 10343...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7368 87936 40494 490286 856005 641588 145611 1...</td>\n",
       "      <td>816903 1012629 957974 1033823 328210 947200 65...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>299237 760651 299237 887082 159592 556634 7489...</td>\n",
       "      <td>563568 1239563 680125 780219 782805 1033823 19...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7368 7368 7368 865510 7368 396966 995243 37685...</td>\n",
       "      <td>816903 816903 816903 139132 816903 312320 1103...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>7368 1160791 299237 1238054 569999 1044285 117...</td>\n",
       "      <td>816903 669476 21577 520477 1004165 4184 616471...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>893673 7368 836872 674898 231468 856005 105964...</td>\n",
       "      <td>277781 816903 1098157 986174 1033823 780491 10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1122654 125310 907560 1172361 979583 983951 12...</td>\n",
       "      <td>289186 640942 363388 585102 261174 1217680 520...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>793790 599682 1223643 1030656 569999 178976 45...</td>\n",
       "      <td>1257015 966562 1054308 599826 811205 520477 28...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>7368 1120647 360394 79747 1140778 472252 7368 ...</td>\n",
       "      <td>816903 266069 1226448 1276450 816903 769051 12...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            article  \\\n",
       "0   0  7368 1252069 365865 755561 1044285 129532 1053...   \n",
       "1   1  581131 165432 7368 957317 1197553 570900 33659...   \n",
       "2   2  7368 87936 40494 490286 856005 641588 145611 1...   \n",
       "3   3  299237 760651 299237 887082 159592 556634 7489...   \n",
       "4   4  7368 7368 7368 865510 7368 396966 995243 37685...   \n",
       "5   5  7368 1160791 299237 1238054 569999 1044285 117...   \n",
       "6   6  893673 7368 836872 674898 231468 856005 105964...   \n",
       "7   7  1122654 125310 907560 1172361 979583 983951 12...   \n",
       "8   8  793790 599682 1223643 1030656 569999 178976 45...   \n",
       "9   9  7368 1120647 360394 79747 1140778 472252 7368 ...   \n",
       "\n",
       "                                            word_seg  class  \n",
       "0  816903 597526 520477 1179558 1033823 758724 63...     14  \n",
       "1  90540 816903 441039 816903 569138 816903 10343...      3  \n",
       "2  816903 1012629 957974 1033823 328210 947200 65...     12  \n",
       "3  563568 1239563 680125 780219 782805 1033823 19...     13  \n",
       "4  816903 816903 816903 139132 816903 312320 1103...     12  \n",
       "5  816903 669476 21577 520477 1004165 4184 616471...     13  \n",
       "6  277781 816903 1098157 986174 1033823 780491 10...      1  \n",
       "7  289186 640942 363388 585102 261174 1217680 520...     10  \n",
       "8  1257015 966562 1054308 599826 811205 520477 28...     10  \n",
       "9  816903 266069 1226448 1276450 816903 769051 12...     19  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  2, 12,  2,  8, 12,  2, 12,  2, 12,  7,  7, 14, 12, 14,  2,  8,\n",
       "        8,  2, 13,  8,  2,  8,  8, 12, 14,  2, 14, 12,  2, 12,  7,  2, 14,\n",
       "       12,  7, 12, 12,  2, 12,  7, 12, 13, 14,  2, 12, 12, 13, 18,  5, 14,\n",
       "       12, 12,  8,  2,  8,  2, 12, 14,  7, 12, 12, 13,  2,  8,  2,  2, 13,\n",
       "       12, 14,  7, 12,  2, 12,  2, 12, 13, 12, 12, 12, 12, 12,  2, 12,  2,\n",
       "       12,  2,  2,  7, 12, 12,  7,  2,  2,  2,  7, 12, 13, 12,  2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MNB()\n",
    "clf.fit(trn_term_doc, y)\n",
    "res = clf.predict(test_term_doc)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "支持向量机（Support Vector Machine, SVM）是一类按监督学习（supervised learning）方式对数据进行二元分类的广义线性分类器，其决策边界是对学习样本求解的最大边距超平面.\n",
    "\n",
    "一般SVM有下面三种：\n",
    "硬间隔支持向量机（线性可分支持向量机）：当训练数据线性可分时，可通过硬间隔最大化学得一个线性可分支持向量机。\n",
    "软间隔支持向量机：当训练数据近似线性可分时，可通过软间隔最大化学得一个线性支持向量机。\n",
    "非线性支持向量机：当训练数据线性不可分时，可通过核方法以及软间隔最大化学得一个非线性支持向量机。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2, 12,  2,  2, 12,  2, 12,  2, 12,  7, 12, 14, 12, 14,  2,  8,\n",
       "        8,  2, 13,  8,  2,  8,  8,  2, 14,  2,  2,  2,  2, 12,  2,  2, 14,\n",
       "       13,  7, 12, 12,  2, 12,  2, 12, 13, 14,  2,  2,  2, 13,  2,  5,  2,\n",
       "        2, 12,  8,  2,  8,  2, 12,  2, 12, 12, 12, 13,  2,  8,  2,  2, 13,\n",
       "       12, 14,  7, 12,  2, 12,  2, 12, 13,  2,  2, 12, 12, 12,  2,  2,  2,\n",
       "       12,  2,  2,  7, 12, 12,  7,  2,  2,  2,  7, 12, 13, 12,  2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel=\"linear\", C=0.4)\n",
    "clf.fit(trn_term_doc, y)\n",
    "res = clf.predict(test_term_doc)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLSA\n",
    "\n",
    "1、朴素贝叶斯的分析\n",
    "\n",
    "（1）可以胜任许多文本分类问题。\n",
    "（2）无法解决语料中一词多义和多词一义的问题——它更像是词法分析，而非语义分析。\n",
    "（3）如果使用词向量作为文档的特征，一词多义和多词一义会造成计算文档间相似度的不准确性。\n",
    "（4）可以通过增加“主题”的方式，一定程度的解决上述\n",
    "问题：一个词可能被映射到多个主题中（一词多义），多个词可能被映射到某个主题的概率很高（多词一义）\n",
    "\n",
    "2.pLSA模型\n",
    "\n",
    "基于概率统计的pLSA模型(probabilistic latentsemantic analysis, 概率隐语义分析)，增加了主题模型，\n",
    "形成简单的贝叶斯网络，可以使用EM算法学习模型参数.\n",
    "\n",
    "### 共轭先验分布\n",
    "\n",
    "1）由于x为给定样本，P(x)有时被称为“证据”，仅仅是归一化因子，如果不关心P(θ|x)的具体值，只考察θ取何值时\n",
    "\n",
    "后验概率P(θ|x)最大，则可将分母省去。\n",
    "\n",
    "![公式](https://ws2.sinaimg.cn/large/006tKfTcly1g0wmgd9ticj30ea02rglm.jpg)\n",
    "\n",
    "在贝叶斯概率理论中，如果后验概率P(θ|x)和先验概率p(θ)满足同样的分布律，那么，先验分布和后验分布被叫做共\n",
    "\n",
    "轭分布，同时，先验分布叫做似然函数的共轭先验分布。\n",
    "\n",
    "2）共轭先验分布的实践意义\n",
    "\n",
    "似然函数P(x|θ)表示以先验θ为参数的概率分布，可以直接求得。 先验分布P(θ)是θ的分布率，可根据先验知识获得。\n",
    "\n",
    "方案：选取似然函数P(x|θ)的共轭先验作为P(θ)的分布，这样，P(x|θ)乘以P(θ) (然后归一化)得到的P(θ|x)的形式和P(θ)的形式一样。\n",
    "\n",
    "### LDA\n",
    "\n",
    "共有m篇文章，一共涉及了K个主题；每篇文章(长度为N m )都有各自的主题分布，主题分布是多项分布，该多项分布的参数服从Dirichlet分布，Dirichlet分布的参数为α；\n",
    "每个主题都有各自的词分布，词分布为多项分布，该多项分布的参数服从Dirichlet分布，该Dirichlet分布的参数为 β;\n",
    "对于某篇文章中的第n个词，首先从该文章的主题分布中采样一个主题，然后在这个主题对应的词分布中采样一个词。不断重复这个随机生成过程，直到m篇文章全部完成上述过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "44 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "2 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "17 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "3 0.476*\"system\" + 0.241*\"human\" + 0.241*\"eps\" + 0.005*\"user\" + 0.005*\"graph\" + 0.005*\"trees\" + 0.005*\"computer\" + 0.005*\"interface\" + 0.005*\"response\" + 0.005*\"time\"\n",
      "7 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "0 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "25 0.315*\"user\" + 0.315*\"response\" + 0.315*\"time\" + 0.006*\"system\" + 0.006*\"graph\" + 0.006*\"trees\" + 0.006*\"eps\" + 0.006*\"computer\" + 0.006*\"survey\" + 0.006*\"human\"\n",
      "31 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "41 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import hdpmodel, ldamodel\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "              \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "all_tokens = sum(texts, [])\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "texts = [[word for word in text if word not in tokens_once]\n",
    "         for text in texts]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "# I can print out the documents and which is the most probable topics for each doc.\n",
    "lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50)\n",
    "\n",
    "for i in lda.show_topics():\n",
    "    print(i[0], i[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
