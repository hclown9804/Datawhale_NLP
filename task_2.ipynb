{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词\n",
    "1、正向最大匹配法：最大匹配是指以词典为依据，取词典中最长单词为第一个次取字数量的扫描串，在词典中进行扫描（为提升扫描效率，还可以跟据字数多少设计多个字典，然后根据字数分别从不同字典中进行扫描）。\n",
    "\n",
    "2、逆向最大匹配法：即从后往前取词，类比正向最大匹配法。\n",
    "\n",
    "3、双向最大匹配法：双向最大匹配法是将正向最大匹配法得到的分词结果和逆向最大匹配法的到的结果进行比较，从而决定正确的分词方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram\n",
    "\n",
    "N-Gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。\n",
    "\n",
    "每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。\n",
    "\n",
    "该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。\n",
    "\n",
    "unigram:一元模型\n",
    "bigram:二元模型\n",
    "trigram:三元模型\n",
    "\n",
    "参考链接：[自然语言处理中N-Gram模型介绍](https://zhuanlan.zhihu.com/p/32829048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./cnews/cnews.train.txt\",sep='\\t',engine='python',names=['label','content'],encoding='UTF-8')\n",
    "test_data = pd.read_csv(\"./cnews/cnews.test.txt\",sep='\\t',engine='python',names=['label','content'],encoding='UTF-8')\n",
    "val_data = pd.read_csv(\"./cnews/cnews.val.txt\",sep='\\t',engine='python',names=['label','content'],encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>体育</td>\n",
       "      <td>商瑞华首战复仇心切 中国玫瑰要用美国方式攻克瑞典多曼来了，瑞典来了，商瑞华首战求3分的信心也...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>体育</td>\n",
       "      <td>冠军球队迎新欢乐派对 黄旭获大奖张军赢下PK赛新浪体育讯12月27日晚，“冠军高尔夫球队迎新...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>体育</td>\n",
       "      <td>辽足签约危机引注册难关 高层威逼利诱合同笑里藏刀新浪体育讯2月24日，辽足爆发了集体拒签风波...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>体育</td>\n",
       "      <td>揭秘谢亚龙被带走：总局电话骗局 复制南杨轨迹体坛周报特约记者张锐北京报道  谢亚龙已经被公安...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content\n",
       "0    体育  马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有...\n",
       "1    体育  商瑞华首战复仇心切 中国玫瑰要用美国方式攻克瑞典多曼来了，瑞典来了，商瑞华首战求3分的信心也...\n",
       "2    体育  冠军球队迎新欢乐派对 黄旭获大奖张军赢下PK赛新浪体育讯12月27日晚，“冠军高尔夫球队迎新...\n",
       "3    体育  辽足签约危机引注册难关 高层威逼利诱合同笑里藏刀新浪体育讯2月24日，辽足爆发了集体拒签风波...\n",
       "4    体育  揭秘谢亚龙被带走：总局电话骗局 复制南杨轨迹体坛周报特约记者张锐北京报道  谢亚龙已经被公安..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 马/ 晓/ 旭/ 意外/ 受伤/ 让/ 国奥/ 警惕/ / / 无奈/ 大雨/ 格外/ 青睐/ 殷家/ 军/ 记者/ 傅/ 亚/ 雨/ 沈阳/ 报道/ / / 来到/ 沈阳/ / / 国奥/ 国奥队/ 依然/ 没有/ 摆脱/ 雨水/ 的/ 困扰/ / 7/ 月/ 31/ 日/ 下午/ 6/ 点/ / / 国奥/ 国奥队/ 的/ 日常/ 训练/ 再度/ 受到/ 大雨/ 的/ 干扰/ / / 无奈/ 之下/ 队员/ 们/ 只/ 慢跑/ 了/ 25/ 分钟/ 就/ 草草/ 草草收场/ 收场/ / 31/ 日/ 上午/ 10/ 点/ / / 国奥/ 国奥队/ 在/ 奥体/ 奥体中心/ 中心/ 外场/ 训练/ 的/ 时候/ / / 天/ 就是/ 阴沉/ 阴沉沉/ 沉沉/ 沉沉的/ / / 气象/ 气象预报/ 预报/ 显示/ 当天/ 天下/ 下午/ 沈阳/ 就/ 有/ 大雨/ / / 但/ 幸好/ 队伍/ 上午/ 的/ 训练/ 并/ 没有/ 受到/ 到任/ 任何/ 何干/ 干扰/ / / 下午/ 6/ 点/ / / 当/ 球队/ 抵达/ 训练/ 训练场/ 时/ / / 大雨/ 已经/ 下/ 了/ 几个/ 小时/ / / 而且/ 丝毫/ 没有/ 停下/ 停下来/ 下来/ 的/ 意思/ / / 抱/ 着/ 试一试/ 的/ 态度/ / / 球队/ 开始/ 了/ 当天/ 天下/ 下午/ 的/ 例行/ 训练/ / 25/ 分钟/ 过去/ 了/ / / 天气/ 没有/ 任何/ 转/ 好/ 的/ 迹象/ / / 为了/ 保护/ 球员/ 们/ / / 国奥/ 国奥队/ 决定/ 中止/ 当天/ 的/ 训练/ / / 全队/ 立即/ 返回/ 酒店/ / / 在/ 雨/ 中/ 训练/ 对/ 足球/ 足球队/ 球队/ 来说/ 并/ 不是/ 什么/ 稀罕/ 罕事/ / / 但/ 在/ 奥运/ 奥运会/ 即将/ 开始/ 之前/ / / 全队/ 变得/ / / 娇贵/ / / 了/ / / 在/ 沈阳/ 最后/ 一周/ 的/ 训练/ / / 国奥/ 国奥队/ 首先/ 先要/ 保证/ 现有/ 的/ 球员/ 不再/ 出现/ 出现意外/ 意外/ 的/ 伤病/ 病情/ 情况/ 以免/ 影响/ 正式/ 比赛/ / / 因此/ 这/ 一/ 阶段/ 控制/ 训练/ 受伤/ / / 控制/ 感冒/ 等/ 疾病/ 的/ 出现/ 被/ 队伍/ 放在/ 了/ 相当/ 重要/ 的/ 位置/ / / 而/ 抵达/ 沈阳/ 之后/ / / 中/ 后卫/ 冯/ 萧/ 霆/ 就/ 一直/ 没有/ 训练/ / / 冯/ 萧/ 霆/ 是/ 7/ 月/ 27/ 日/ 在/ 长春/ 患上/ 了/ 感冒/ / / 因此/ 也/ 没有/ 参加/ 29/ 日/ 跟/ 塞尔/ 塞尔维/ 塞尔维亚/ 维亚/ 的/ 热身/ 热身赛/ / / 队伍/ 介绍/ 说/ / / 冯/ 萧/ 霆/ 并/ 没有/ 出现/ 发烧/ 症状/ / / 但/ 为了/ 安全/ 起见/ / / 这/ 两天/ 还是/ 让/ 他/ 静养/ 休息/ / / 等/ 感冒/ 彻底/ 好/ 了/ 之后/ 再/ 恢复/ 复训/ 训练/ / / 由于/ 有/ 了/ 冯/ 萧/ 霆/ 这个/ 个例/ 例子/ / / 因此/ 国奥/ 国奥队/ 对/ 雨/ 中/ 训练/ 练就/ 显得/ 特别/ 谨慎/ / / 主要/ 要是/ 担心/ 球员/ 们/ 受凉/ 而/ 引发/ 感冒/ / / 造成/ 非战/ 非战斗/ 战斗/ 减员/ / / 而/ 女足/ 队员/ 马/ 晓/ 旭/ 在/ 热身/ 热身赛/ 中/ 受伤/ 导致/ 无缘/ 奥运/ 的/ 前科/ / / 也/ 让/ 在/ 沈阳/ 的/ 国奥/ 国奥队/ 现在/ 格外/ 警惕/ / / / 训练/ 中/ 不断/ 嘱咐/ 队员/ 们/ 要/ 注意/ 意动/ 动作/ / / 我们/ 可不/ 不能/ 再/ 出/ 这样/ 的/ 事情/ 了/ / / / 一位/ 工作/ 工作人员/ 作人/ 人员/ 表示/ / / 从/ 长春/ 到/ 沈阳/ / / 雨水/ 一路/ 伴随/ 随着/ 国奥/ 国奥队/ / / / 也/ 邪/ 了/ / / 我们/ 走到/ 哪儿/ 雨/ 就/ 下到/ 哪儿/ / / 在/ 长春/ 几次/ 训练/ 都/ 被/ 大雨/ 给/ 搅和/ 了/ / / 没想/ 没想到/ 想到/ 到来/ 沈阳/ 又/ 碰到/ 这种/ 事情/ / / / 一位/ 国奥/ 球员/ 也/ 对/ 雨水/ 的/ / / 青睐/ / / 有些/ 不解/ / \n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "seg_list = jieba.cut(train_data[\"content\"][0], cut_all=False)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取停用词\n",
    "stopwords = []\n",
    "with open('stopwords.txt', 'r') as fr:\n",
    "    for line in fr:\n",
    "        stopwords.append(line[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('训练', 13),\n",
       " ('国奥队', 8),\n",
       " ('沈阳', 8),\n",
       " ('大雨', 5),\n",
       " ('下午', 4),\n",
       " ('中', 4),\n",
       " ('感冒', 4),\n",
       " ('球员', 4),\n",
       " ('日', 4),\n",
       " ('冯萧霆', 4),\n",
       " ('雨水', 3),\n",
       " ('受伤', 3),\n",
       " ('当天', 3),\n",
       " ('队员', 3),\n",
       " ('点', 3),\n",
       " ('长春', 3),\n",
       " ('队伍', 3),\n",
       " ('抵达', 2),\n",
       " ('月', 2),\n",
       " ('球队', 2),\n",
       " ('国奥', 2),\n",
       " ('热身赛', 2),\n",
       " ('无奈', 2),\n",
       " ('一位', 2),\n",
       " ('31', 2),\n",
       " ('25', 2),\n",
       " ('6', 2),\n",
       " ('分钟', 2),\n",
       " ('青睐', 2),\n",
       " ('控制', 2),\n",
       " ('上午', 2),\n",
       " ('干扰', 2),\n",
       " ('警惕', 2),\n",
       " ('雨', 2),\n",
       " ('7', 2),\n",
       " ('事情', 2),\n",
       " ('马晓旭', 2),\n",
       " ('全队', 2),\n",
       " ('例子', 1),\n",
       " ('再度', 1),\n",
       " ('保护', 1),\n",
       " ('疾病', 1),\n",
       " ('非战斗', 1),\n",
       " ('介绍', 1),\n",
       " ('例行', 1),\n",
       " ('报道', 1),\n",
       " ('现有', 1),\n",
       " ('日常', 1),\n",
       " ('之下', 1),\n",
       " ('29', 1),\n",
       " ('几个', 1),\n",
       " ('嘱咐', 1),\n",
       " ('参加', 1),\n",
       " ('担心', 1),\n",
       " ('再出', 1),\n",
       " ('依然', 1),\n",
       " ('正式', 1),\n",
       " ('训练场', 1),\n",
       " ('恢复', 1),\n",
       " ('阴沉沉', 1),\n",
       " ('显示', 1),\n",
       " ('走', 1),\n",
       " ('10', 1),\n",
       " ('转好', 1),\n",
       " ('困扰', 1),\n",
       " ('出现意外', 1),\n",
       " ('抱', 1),\n",
       " ('迹象', 1),\n",
       " ('停下来', 1),\n",
       " ('前科', 1),\n",
       " ('态度', 1),\n",
       " ('幸好', 1),\n",
       " ('军', 1),\n",
       " ('这一', 1),\n",
       " ('来到', 1),\n",
       " ('草草收场', 1),\n",
       " ('女足', 1),\n",
       " ('动作', 1),\n",
       " ('事', 1),\n",
       " ('意外', 1),\n",
       " ('天', 1),\n",
       " ('变得', 1),\n",
       " ('搅和', 1),\n",
       " ('谨慎', 1),\n",
       " ('天气', 1),\n",
       " ('塞尔维亚', 1),\n",
       " ('保证', 1),\n",
       " ('无缘', 1),\n",
       " ('比赛', 1),\n",
       " ('对雨中', 1),\n",
       " ('说', 1),\n",
       " ('休息', 1),\n",
       " ('患上', 1),\n",
       " ('静养', 1),\n",
       " ('特别', 1),\n",
       " ('影响', 1),\n",
       " ('显得', 1),\n",
       " ('试一试', 1),\n",
       " ('症状', 1),\n",
       " ('殷家', 1),\n",
       " ('伴随', 1),\n",
       " ('足球队', 1),\n",
       " ('导致', 1),\n",
       " ('阶段', 1),\n",
       " ('一路', 1),\n",
       " ('碰到', 1),\n",
       " ('奥运', 1),\n",
       " ('邪', 1),\n",
       " ('小时', 1),\n",
       " ('记者', 1),\n",
       " ('摆脱', 1),\n",
       " ('傅亚雨', 1),\n",
       " ('27', 1),\n",
       " ('不解', 1),\n",
       " ('返回', 1),\n",
       " ('减员', 1),\n",
       " ('情况', 1),\n",
       " ('几次', 1),\n",
       " ('慢跑', 1),\n",
       " ('没想到', 1),\n",
       " ('奥运会', 1),\n",
       " ('伤病', 1),\n",
       " ('位置', 1),\n",
       " ('受凉', 1),\n",
       " ('中止', 1),\n",
       " ('放在', 1),\n",
       " ('奥体中心', 1),\n",
       " ('丝毫', 1),\n",
       " ('发烧', 1),\n",
       " ('酒店', 1),\n",
       " ('工作人员', 1),\n",
       " ('两天', 1),\n",
       " ('时', 1),\n",
       " ('外场', 1),\n",
       " ('气象预报', 1),\n",
       " ('稀罕', 1),\n",
       " ('引发', 1),\n",
       " ('后卫', 1),\n",
       " ('一周', 1),\n",
       " ('娇贵', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算词频\n",
    "seg_list = jieba.cut(train_data[\"content\"][0], cut_all=False)\n",
    "result = []\n",
    "for seg in seg_list:\n",
    "    seg = ''.join(seg.split())\n",
    "    if seg != '' and seg != \"\\n\" and seg != \"\\n\\n\" and seg not in stopwords:\n",
    "        result.append(seg)\n",
    "word_fre = {}\n",
    "for i in set(result):\n",
    "    word_fre[i] = result.count(i)\n",
    "word_fre = sorted(word_fre.items(), key=lambda item: item[1], reverse=True)\n",
    "word_fre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 分词\n",
    "train_data = train_data[:5]\n",
    "words = []\n",
    "for sentence in train_data['content']:\n",
    "    word = list(jieba.cut(sentence))\n",
    "    for w in list(set(word) and set(stopwords)):\n",
    "        while w in word:\n",
    "            word.remove(w)\n",
    "    words.append(' '.join(word))\n",
    "train_data['content_'] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0,  0,  0,  2,  1,  2,  4,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  1,  1,  0,  1,  3,  0,  1,  0,  1,  1,  1,  1,\n",
       "         0,  3,  1,  0,  1,  1,  0,  0,  2,  0,  0,  0,  0,  0,  2,  1,\n",
       "         0,  1,  0,  1,  1,  8,  0,  2,  0,  4,  2,  0,  0,  0, 13,  1,\n",
       "         0,  0,  0,  1,  0,  0,  3,  1],\n",
       "       [ 0,  0,  1,  1,  1,  0,  0,  0,  0,  1,  1,  3,  5,  1,  1,  1,\n",
       "         0,  3,  0,  2,  0,  1,  0,  0,  1,  1,  1,  1,  3,  3,  1,  1,\n",
       "         0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  0,  2,  0,  0,\n",
       "         1,  2,  0,  0,  4,  0,  0,  2,  1,  2,  0,  0,  1,  0,  1,  3,\n",
       "         1,  0,  0,  0,  1,  1,  1,  1],\n",
       "       [ 1,  1,  0,  0,  0,  0,  1,  0,  0,  1,  1,  0,  1,  0,  1,  1,\n",
       "         1,  0,  4,  0,  0,  0,  1,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         1,  0,  0,  1,  0,  0,  0,  2,  1,  5,  1,  3,  1,  0,  3,  1,\n",
       "         1,  0,  2,  2,  0,  0,  0,  0,  3,  0,  3,  1,  0,  1,  2,  0,\n",
       "         0,  0,  0,  1,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1,  2,  1,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,\n",
       "         1,  1, 16,  0,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  1,  1,  2,  0,  1,  2,  0,  1,  0,  2,  1,  0,  0,  0,\n",
       "         0,  0,  1,  0,  1,  0,  1,  0,  0, 13,  4,  1,  1,  0,  0,  0,\n",
       "         0,  1,  1,  0,  1,  0,  0,  1],\n",
       "       [ 1,  1,  1,  0,  0,  0,  0,  1,  2,  0,  0,  1,  0,  0,  0,  0,\n",
       "         0,  0,  0,  2,  0,  0,  1,  2,  0,  0,  0,  2,  0,  0,  0,  0,\n",
       "         1,  1,  0,  0,  1,  2,  0,  0,  0,  0,  1,  0,  0,  1,  0,  0,\n",
       "         1,  1,  0,  2,  0,  1,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,\n",
       "        17,  3,  3,  0,  1,  2,  0,  0]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算 TF-IDF\n",
    "count_vectorizer = CountVectorizer(max_features=256, min_df=2)\n",
    "count_vectorizer.fit_transform(train_data['content_'])\n",
    "fea_vec = count_vectorizer.transform(train_data['content_']).toarray()\n",
    "fea_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
