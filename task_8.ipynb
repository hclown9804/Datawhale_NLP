{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "在日常生活中有许多这样的问题，当顺序被打乱时，它们会被完全打乱。例如，\n",
    "\n",
    "我们之前看到的语言——单词的顺序定义了它们的意义;\n",
    "时间序列数据——时间定义了事件的发生;\n",
    "基因组序列数据——每个序列都有不同的含义.\n",
    "\n",
    "有很多这样的情况，序列的信息决定事件本身。如果我们试图使用这类数据得到有用的输出，就需要一个这样的网络：能够访问一些关于数据的先前知识（prior knowledge），以便完全理解这些数据。因此，循环神经网络（RNN）出现。\n",
    "\n",
    "\n",
    "简单RNN，由输入层、一个隐藏层和一个输出层构成。\n",
    "![pic1](https://ws2.sinaimg.cn/large/006tKfTcly1g15o2mmyehj303s08vmx3.jpg)\n",
    "\n",
    "x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的权重矩阵；o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。那么，现在我们来看看W是什么。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。\n",
    "\n",
    "将上图展开：\n",
    "![pic2](https://ws3.sinaimg.cn/large/006tKfTcly1g15o3l0q3zj30m308vq3h.jpg)\n",
    "\n",
    "### RNN前向传播\n",
    "\n",
    "![pic3](https://ws2.sinaimg.cn/large/006tKfTcly1g15of5wbwij30gb088q45.jpg)\n",
    "\n",
    "### RNN反向传播\n",
    "\n",
    "![pic4](https://ws4.sinaimg.cn/large/006tKfTcly1g15oe73bhcj30ja0oa791.jpg)\n",
    "\n",
    "### RNN出现的问题\n",
    "\n",
    "RNN基于这样的机制，信息的结果依赖于前面的状态或前N个时间步。普通的RNN可能在学习长距离依赖性方面存在困难。\n",
    "如果我们在这种情况下后向传播，我们就需要应用链式法则。如果任何一个梯度接近0，所有的梯度都会成指数倍的迅速变成零。这样将不再有助于网络学习任何东西。这就是所谓的消失梯度问题。\n",
    "\n",
    "合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。\n",
    "使用relu代替sigmoid和tanh作为激活函数。\n",
    "使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU）。\n",
    "\n",
    "### 双向RNN\n",
    "\n",
    "对于语言模型来说，很多时候光看前面的词是不够的，比如下面这句话：\n",
    "\n",
    "我的手机坏了，我打算____一部新手机。\n",
    "\n",
    "可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是『一部新手机』，那么，横线上的词填『买』的概率就大得多了。\n",
    "\n",
    "因此我们需要双向RNN：\n",
    "![pic5](https://ws4.sinaimg.cn/large/006tKfTcly1g15ojjuoknj31080cogn9.jpg)\n",
    "\n",
    "双向卷积神经网络的隐藏层要保存两个值，一个A参与正向计算，另一个值A'参与反向计算。最终的输出值y2取决于A2和A2'。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 递归神经网络\n",
    "\n",
    "循环神经网络，它可以用来处理包含序列结构的信息。然而，除此之外，信息往往还存在着诸如树结构、图结构等更复杂的结构。对于这种复杂的结构，循环神经网络就无能为力了。而递归神经网络（巧合的是，它的缩写和循环神经网络一样，也是RNN）可以处理诸如树、图这样的递归结构。\n",
    "\n",
    "递归神经网络可以把一个树/图结构信息编码为一个向量，也就是把信息映射到一个语义向量空间中。这个语义向量空间满足某类性质，比如语义相似的向量距离更近。也就是说，如果两句话（尽管内容不同）它的意思是相似的，那么把它们分别编码后的两个向量的距离也相近；反之，如果两句话的意思截然不同，那么编码后向量的距离则很远。\n",
    "\n",
    "![pic6](https://ws4.sinaimg.cn/large/006tKfTcly1g15onmsg43j30f508nwew.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "长短期记忆网络（LSTM）的全称是Long Short Term Memory networks，是RNN的一种特殊形式，特点是能够学习长距离依赖关系。由Hochreiter & Schmidhuber (1997)首先提出，之后被很多学者改善和推广。它在很多问题上都得到很好的表现，现在被广泛使用。\n",
    "LSTM的设计之初就是为了解决长距离依赖问题。记住长距离的信息实际上是他们的最基本的行为，而不是他们难以学习的东西！\n",
    "所有循环神经网络都具有神经网络重复模块链的形式。在标准RNN中，这个重复模块具有非常简单的结构，例如单个tanh层。\n",
    "\n",
    "![pic7](https://ws4.sinaimg.cn/large/006tKfTcly1g15opt2pzpj30k007h0t3.jpg)\n",
    "\n",
    "LSTM也有这样的链式结构，但重复模块有不同的结构。不是有一个单一的神经网络层，而是有四个，他们之间以一种非常特殊的方式进行交互。\n",
    "![pic8](https://ws1.sinaimg.cn/large/006tKfTcly1g15oqa3pg6j30k007jmxp.jpg)\n",
    "\n",
    "![pic9](https://ws1.sinaimg.cn/large/006tKfTcly1g15oqlvbzkj30k003q0sw.jpg)\n",
    "\n",
    "在上面的图中，一条线包含一个完整的向量，从一个节点的输出到另一个节点的输入。粉色圆圈表示对应元素操作，如向量加法，而黄色框是神经网络层。合并的线表示拼接，而分叉线表示其内容被复制，副本将流转到不同的位置。\n",
    "\n",
    "## GRU\n",
    "\n",
    "RU是新一代RNN，与LSTM非常相似。GRU不使用单元状态，而是使用隐藏状态来传输信息。它也只有两个门，一个重置门和一个更新门.\n",
    "![pic10](https://ws2.sinaimg.cn/large/006tKfTcly1g15otz6ra5j30og0jxta5.jpg)\n",
    "\n",
    "更新门的作用类似于LSTM的遗忘和输入门。它决定要丢弃哪些信息和要添加哪些新信息。\n",
    "\n",
    "重置门是另一个用来决定要忘记多少过去的信息的门。\n",
    "\n",
    "## 针对梯度消失（LSTM等其他门控RNN）、梯度爆炸（梯度截断）的解决方案\n",
    "\n",
    "使用一个合适激活函数，它的梯度在一个合理的范围。LSTM使用gate function，有选择的让一部分信息通过。gate是由一个sigmoid单元和一个逐点乘积操作组成，sigmoid单元输出1或0，用来判断通过还是阻止，然后训练这些gate的组合。所以，当gate是打开的（梯度接近于1），梯度就不会vanish。并且sigmoid不超过1，那么梯度也不会explode。\n",
    "\n",
    "1、当gate是关闭的，那么就会阻止对当前信息的改变，这样以前的依赖信息就会被学到。\n",
    "2、当gate是打开的时候，并不是完全替换之前的信息，而是在之前信息和现在信息之间做加权平均。所以，无论网络的深度有多深，输入序列有多长，只要gate是打开的，网络都会记住这些信息。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRNN\n",
    "\n",
    "假设训练集中所有文本/序列的长度统一为n，我们需要对文本进行分词，并使用词嵌入得到每个词固定维度的向量表示。对于每一个输入文本/序列，我们可以在RNN的每一个时间步长上输入文本中一个单词的向量表示，计算当前时间步长上的隐藏状态，然后用于当前时间步骤的输出以及传递给下一个时间步长并和下一个单词的词向量一起作为RNN单元输入，然后再计算下一个时间步长上RNN的隐藏状态，以此重复…直到处理完输入文本中的每一个单词，由于输入文本的长度为n，所以要经历n个时间步长。\n",
    "\n",
    "![pic11](https://ws4.sinaimg.cn/large/006tKfTcly1g1615e5bjgj30c70aqt9d.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRNN\n",
    "\n",
    "![pic14](https://ws4.sinaimg.cn/large/006tKfTcly1g161l82x58j30ll0hxmzz.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCNN原理\n",
    "\n",
    "Region CNN(RCNN)可以说是利用深度学习进行目标检测的开山之作。作者Ross Girshick多次在PASCAL VOC的目标检测竞赛中折桂，2010年更带领团队获得终身成就奖，如今供职于Facebook旗下的FAIR。 \n",
    "\n",
    "经典的目标检测算法使用滑动窗法依次判断所有可能的区域。本文则预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上提取特征，进行判断。\n",
    "经典的目标检测算法在区域中提取人工设定的特征（Haar，HOG）。本文则需要训练深度网络进行特征提取。可供使用的有两个数据库： \n",
    "一个较大的识别库（ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。 \n",
    "一个较小的检测库（PASCAL VOC 2007）：标定每张图片中，物体的类别和位置。一万图像，20类。 \n",
    "本文使用识别库进行预训练，而后用检测库调优参数。最后在检测库上评测。\n",
    "\n",
    "RCNN算法分为4个步骤：\n",
    "一张图像生成1K~2K个候选区域;\n",
    "对每个候选区域，使用深度网络提取特征;\n",
    "特征送入每一类的SVM 分类器，判别是否属于该类; \n",
    "使用回归器精细修正候选框位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
