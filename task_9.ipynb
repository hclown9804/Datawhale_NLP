{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "Attention是一种用于提升基于RNN（LSTM或GRU）的Encoder + Decoder模型的效果的的机制（Mechanism），一般称为Attention Mechanism。Attention Mechanism目前非常流行，广泛应用于机器翻译、语音识别、图像标注（Image Caption）等很多领域，之所以它这么受欢迎，是因为Attention给模型赋予了区分辨别的能力，例如，在机器翻译、语音识别应用中，为句子中的每个词赋予不同的权重，使神经网络模型的学习变得更加灵活（soft），同时Attention本身可以做为一种对齐关系，解释翻译输入/输出句子之间的对齐关系，解释模型到底学到了什么知识，为我们打开深度学习的黑箱，提供了一个窗口。\n",
    "\n",
    "\n",
    "![pic1](https://ws2.sinaimg.cn/large/006tKfTcly1g17a83f016j30go0a8myp.jpg)\n",
    "\n",
    "一般我们会使用CNN或者RNN（包括GRU或者LSTM）等模型来对序列数据进行编码，然后采用各种pooling或者对RNN直接取最后一个t时刻的hidden state作为句子的向量输出。这里会有一个问题：常规的编码方法，无法体现对一个句子序列中不同语素的关注程度，在自然语言中，一个句子中的不同部分是有不同含义和重要性的，比如上面的例子中：I hate this movie.如果做情感分析，明显对hate这个词语应当关注更多。当然是用CNN和RNN能够编码这种信息。但是如果序列长度很长的情况下，这种方法会有一定的瓶颈。\n",
    "\n",
    "![pic2](https://ws3.sinaimg.cn/large/006tKfTcly1g17a8vpry4j30go06qaar.jpg)\n",
    "\n",
    "CNN的核心就是卷积核能够变相学习n-gram的信息，如果是用hierarchical的卷积核，那么越上层的卷积核越能编码原始距离较远的词组的信息。但是这种编码能力也是有上限的，对于较长的文本，模型效果不会再提升太多。RNN也是同理。\n",
    "\n",
    "首先讲一下attention最基本最抽象的流程。以seq2seq模型为例。\n",
    "基本流程：对于一个句子序列S，其由单词序列[w1,w2,w3,...,wn]构成。\n",
    "1、应用某种方法S的每个单词$W_{i}$编码为一个单独向量$V_{i}$。\n",
    "2、解码时，使用学习到的注意力权重$a_{i}$对中得到的所有单词向量做加权线性组合.\n",
    "3、在decoder进行下一个词的预测时，使用2中得到的线性组合。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAN\n",
    "\n",
    "HAN的网络结构如图所示，它的核心结构由两个部分组成，下面是一个单词编码器加上基于单词编码的Attention层，上面是一个句子编码器和一个基于句子编码的Attention层。\n",
    "\n",
    "![pic3](https://ws2.sinaimg.cn/large/006tKfTcly1g17ajzsu0rj309e0b1aan.jpg)\n",
    "\n",
    "图1中最底下的部分是个单词编码器，它的输入是一个句子。给定一个由单词$w_{it}$组成的句子$T_{i}$ ，它首先会经过一个嵌入矩阵编码成一个特征向量，例如word2vec等方法：\n",
    "\n",
    "![pic4](https://ws1.sinaimg.cn/large/006tKfTcly1g17amff1slj30jb09674z.jpg)\n",
    "\n",
    "### 单词Attention\n",
    "\n",
    "![pic5](https://ws2.sinaimg.cn/large/006tKfTcly1g17anmebv8j30jd0cxwgu.jpg)\n",
    "\n",
    "### 句子编码器\n",
    "\n",
    "![pic6](https://ws1.sinaimg.cn/large/006tKfTcly1g17ao0ck4cj30ie0653yy.jpg)\n",
    "\n",
    "### 句子Attention\n",
    "\n",
    "![pic7](https://ws2.sinaimg.cn/large/006tKfTcly1g17aocdumgj30iy07i74x.jpg)\n",
    "\n",
    "### 句子分类\n",
    "\n",
    "![pic8](https://ws4.sinaimg.cn/large/006tKfTcly1g17aoqpjqcj30ie06t0tg.jpg)\n",
    "\n",
    "结合要解决的问题的具体内容设计与之对应的算法流程和网络结构是一个合格算法工程师必须要掌握的技能之一。本文做了很好的示范，它在文档分类任务中将一个文档按照句子和单词进行了分层，并且在每层中使用了效果非常好的注意力机制。通过层次的注意力机制我们可以分析每个单词，每个句子在文档分类中扮演的作用，这对我们理解模型是非常有帮助的。\n",
    "\n",
    "Reference\n",
    "[1] Yang Z, Yang D, Dyer C, et al. Hierarchical attention networks for document classification[C]//Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016: 1480-1489.\n",
    "\n",
    "[2] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.\n",
    "\n",
    "[3] Weston J, Chopra S, Bordes A. Memory networks[J]. arXiv preprint arXiv:1410.3916, 2014.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
